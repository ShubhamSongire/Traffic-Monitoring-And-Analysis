{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/deshwalmahesh/yolov7-deepsort-tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We have 3 important files for this purpose and each and every dependency, class, import, function, variable etc is being imported from these modules\n",
    "\n",
    "1. `detection_helpers` which I made to wrap the original `YOLOv-7` code along with helper functions\n",
    "2. `tracking_helpers` has modular code which is used to wrap the `DeepSORT` repo and workings\n",
    "3. `bridge_wrapper` acts as a bridge to bind **ANY** detection model with `DeepSORT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from detection_helpers import *\n",
    "from tracking_helpers import *\n",
    "from  bridge_wrapper import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection\n",
    "Detect objects using `Yolov-7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "\n",
    "Works as follows:\n",
    "1. Read each frame of video using `OpenCV`\n",
    "2. Get Bounding Box or Detections from the model per frame\n",
    "3. Crop those patches and pass on to `reID` model for re identification which is a part of `DeepSORT` method\n",
    "4. Get the above embeddings and then use `Kalman Filtering` and `Hungerian assignment` to assign the correct BB to the respective object\n",
    "5. Show, Save the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBS05\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SBS05\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(classes = [0,17,32]) # it'll detect ONLY [person,horses,sports ball]. class = None means detect all classes. List info at: \"data/coco.yaml\"\n",
    "# detector.load_model('./weights/yolov7x.pt',) # pass the path to the trained weight file\n",
    "detector.load_model(r\"E:\\downloads\\yolov7x.pt\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame no: 1 || Current FPS: 0.61 || Objects tracked: 19\n",
      "Processed frame no: 2 || Current FPS: 0.57 || Objects tracked: 20\n",
      "Processed frame no: 3 || Current FPS: 0.68 || Objects tracked: 20\n",
      "Processed frame no: 4 || Current FPS: 0.71 || Objects tracked: 20\n",
      "Processed frame no: 5 || Current FPS: 0.67 || Objects tracked: 19\n",
      "Processed frame no: 6 || Current FPS: 0.77 || Objects tracked: 19\n",
      "Processed frame no: 7 || Current FPS: 0.66 || Objects tracked: 19\n",
      "Processed frame no: 8 || Current FPS: 0.77 || Objects tracked: 19\n",
      "Processed frame no: 9 || Current FPS: 0.64 || Objects tracked: 19\n",
      "Processed frame no: 10 || Current FPS: 0.81 || Objects tracked: 19\n",
      "Processed frame no: 11 || Current FPS: 0.63 || Objects tracked: 20\n",
      "Processed frame no: 12 || Current FPS: 0.76 || Objects tracked: 19\n",
      "Processed frame no: 13 || Current FPS: 0.64 || Objects tracked: 19\n"
     ]
    }
   ],
   "source": [
    "# Initialise  class that binds detector and tracker in one class\n",
    "tracker = YOLOv7_DeepSORT(reID_model_path=\"./deep_sort/model_weights/mars-small128.pb\", detector=detector)\n",
    "\n",
    "# output = None will not save the output video\n",
    "tracker.track_video(\"0\", show_live = True, skip_frames = 0, count_objects = True, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepSORT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb8ede6e48f2442e28c0595893d2b9fa1cb763a73b1e5669a97c4ab5fc34a251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
